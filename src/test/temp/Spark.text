Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/11 18:26:45 INFO SparkContext: Running Spark version 2.0.1
16/10/11 18:26:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/11 18:26:49 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.114.132 instead (on interface ens33)
16/10/11 18:26:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/11 18:26:49 INFO SecurityManager: Changing view acls to: adolphor
16/10/11 18:26:49 INFO SecurityManager: Changing modify acls to: adolphor
16/10/11 18:26:49 INFO SecurityManager: Changing view acls groups to:
16/10/11 18:26:49 INFO SecurityManager: Changing modify acls groups to:
16/10/11 18:26:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(adolphor); groups with view permissions: Set(); users  with modify permissions: Set(adolphor); groups with modify permissions: Set()
16/10/11 18:26:51 INFO Utils: Successfully started service 'sparkDriver' on port 41840.
16/10/11 18:26:51 INFO SparkEnv: Registering MapOutputTracker
16/10/11 18:26:51 INFO SparkEnv: Registering BlockManagerMaster
16/10/11 18:26:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd73bb4d-1187-4a41-9027-36cced3294f2
16/10/11 18:26:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/11 18:26:51 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/11 18:26:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/11 18:26:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.114.132:4040
16/10/11 18:26:52 INFO SparkContext: Added JAR file:/home/adolphor/java/Spark/spark-2.0.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.0.1.jar at spark://192.168.114.132:41840/jars/spark-examples_2.11-2.0.1.jar with timestamp 1476235612835
16/10/11 18:26:52 INFO SparkContext: Added JAR file:/home/adolphor/java/Spark/spark-2.0.1-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar at spark://192.168.114.132:41840/jars/scopt_2.11-3.3.0.jar with timestamp 1476235612836
16/10/11 18:26:53 INFO Executor: Starting executor ID driver on host localhost
16/10/11 18:26:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45929.
16/10/11 18:26:53 INFO NettyBlockTransferService: Server created on 192.168.114.132:45929
16/10/11 18:26:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.114.132, 45929)
16/10/11 18:26:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.114.132:45929 with 366.3 MB RAM, BlockManagerId(driver, 192.168.114.132, 45929)
16/10/11 18:26:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.114.132, 45929)
16/10/11 18:26:53 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
16/10/11 18:26:54 INFO SharedState: Warehouse path is '/home/adolphor/java/Spark/spark-2.0.1-bin-hadoop2.7/bin/spark-warehouse'.
16/10/11 18:26:55 INFO SparkContext: Starting job: reduce at SparkPi.scala:38
16/10/11 18:26:55 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 2 output partitions
16/10/11 18:26:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)
16/10/11 18:26:55 INFO DAGScheduler: Parents of final stage: List()
16/10/11 18:26:55 INFO DAGScheduler: Missing parents: List()
16/10/11 18:26:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents
16/10/11 18:26:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1832.0 B, free 366.3 MB)
16/10/11 18:26:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1169.0 B, free 366.3 MB)
16/10/11 18:26:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.114.132:45929 (size: 1169.0 B, free: 366.3 MB)
16/10/11 18:26:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/11 18:26:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34)
16/10/11 18:26:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/11 18:26:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5524 bytes)
16/10/11 18:26:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5524 bytes)
16/10/11 18:26:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/11 18:26:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/11 18:26:56 INFO Executor: Fetching spark://192.168.114.132:41840/jars/spark-examples_2.11-2.0.1.jar with timestamp 1476235612835
16/10/11 18:26:56 INFO TransportClientFactory: Successfully created connection to /192.168.114.132:41840 after 124 ms (0 ms spent in bootstraps)
16/10/11 18:26:57 INFO Utils: Fetching spark://192.168.114.132:41840/jars/spark-examples_2.11-2.0.1.jar to /tmp/spark-0bace0a2-e46d-4454-9acb-bd154780154b/userFiles-dba0bcba-2710-4c10-b8e0-ee8dd4707957/fetchFileTemp6036700534940951026.tmp
16/10/11 18:26:57 INFO Executor: Adding file:/tmp/spark-0bace0a2-e46d-4454-9acb-bd154780154b/userFiles-dba0bcba-2710-4c10-b8e0-ee8dd4707957/spark-examples_2.11-2.0.1.jar to class loader
16/10/11 18:26:57 INFO Executor: Fetching spark://192.168.114.132:41840/jars/scopt_2.11-3.3.0.jar with timestamp 1476235612836
16/10/11 18:26:57 INFO Utils: Fetching spark://192.168.114.132:41840/jars/scopt_2.11-3.3.0.jar to /tmp/spark-0bace0a2-e46d-4454-9acb-bd154780154b/userFiles-dba0bcba-2710-4c10-b8e0-ee8dd4707957/fetchFileTemp2379744602233158454.tmp
16/10/11 18:26:57 INFO Executor: Adding file:/tmp/spark-0bace0a2-e46d-4454-9acb-bd154780154b/userFiles-dba0bcba-2710-4c10-b8e0-ee8dd4707957/scopt_2.11-3.3.0.jar to class loader
16/10/11 18:26:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1032 bytes result sent to driver
16/10/11 18:26:57 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1032 bytes result sent to driver
16/10/11 18:26:57 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1130 ms on localhost (1/2)
16/10/11 18:26:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1343 ms on localhost (2/2)
16/10/11 18:26:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
16/10/11 18:26:57 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 1.441 s
16/10/11 18:26:57 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.736904 s
Pi is roughly 3.1427357136785683
16/10/11 18:26:57 INFO SparkUI: Stopped Spark web UI at http://192.168.114.132:4040
16/10/11 18:26:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/11 18:26:58 INFO MemoryStore: MemoryStore cleared
16/10/11 18:26:58 INFO BlockManager: BlockManager stopped
16/10/11 18:26:58 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/11 18:26:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/11 18:26:58 INFO SparkContext: Successfully stopped SparkContext
16/10/11 18:26:58 INFO ShutdownHookManager: Shutdown hook called
16/10/11 18:26:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0bace0a2-e46d-4454-9acb-bd154780154b
